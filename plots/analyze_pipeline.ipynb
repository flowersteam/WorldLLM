{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib\n",
    "import mplcursors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "from worldllm_envs.base import BaseRuleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test trajectories from the env, verify that these are the same as the ones used in the training of the data\n",
    "\n",
    "test_trajectories = BaseRuleEnv.load_test_dataset(\"../worldllm_envs/worldllm_envs/playground/data/test_dataset.json\")\n",
    "# Keep only the scorable observations\n",
    "test_observations = []\n",
    "for trajectory in test_trajectories:\n",
    "    for incr, observation in enumerate(trajectory.lst_diff):\n",
    "        test_observations.append(observation)\n",
    "nb_transitions = len(test_observations)\n",
    "\n",
    "#Load performance without rules\n",
    "with open(\"../outputs/no_rule_ne/2024-10-21/12-12-19_60/all.json\", \"r\") as f:\n",
    "    norule_data = json.load(f)\n",
    "    norule_likelihood_lst = norule_data[\"metrics\"][\"test_likelihoods_best\"]\n",
    "    norule_transition_likelihood_lst = []\n",
    "    for lst_trajectory_likelihood in norule_data[\"metrics\"][\"test_transition_scores_best\"]:\n",
    "        norule_transition_likelihood_lst.append([transition for trajectory_likelihood in lst_trajectory_likelihood for transition in trajectory_likelihood])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "shared_extension = \"_ne_wt3\"\n",
    "baselines_to_load = [\n",
    "    \"baseline_pb\", \"baseline_rb\",\"baseline_cp\",\"baseline_db\", \"pipeline_loglik\", \"pipeline_alp\"\n",
    "]\n",
    "baselines_names = [\"PB\", \"RB\", \"CP\", \"DB\", \"LL\", \"ALP\"]\n",
    "assert len(baselines_to_load) == len(baselines_names)\n",
    "all_data = {k: [] for k in baselines_names}\n",
    "all_rules = {}\n",
    "all_likelihoods = {}\n",
    "all_best_rules = {}\n",
    "all_best_likelihoods = {}\n",
    "all_best_transition_ll = {}\n",
    "all_weights = {}\n",
    "all_additional_data = {}\n",
    "for baseline, b_name in zip(baselines_to_load, baselines_names):\n",
    "    print(\n",
    "        \"-------------------------------- Load \"\n",
    "        + baseline\n",
    "        + \" --------------------------------\"\n",
    "    )\n",
    "    path_start = \"../outputs/\" + baseline + shared_extension + \"/\"\n",
    "    lst_data = []\n",
    "    for f_day in os.listdir(path_start):\n",
    "        if os.path.isdir(path_start + f_day):\n",
    "            lst_dir = sorted(os.listdir(path_start + f_day))\n",
    "            for f_hour in lst_dir:\n",
    "                if os.path.isdir(path_start + f_day + \"/\" + f_hour):\n",
    "                    path = path_start + f_day + \"/\" + f_hour + \"/all.json\"\n",
    "                    with open(path, \"r\") as file:\n",
    "                        lst_data.append(json.load(file))\n",
    "                        print(\"Loaded: \" + path)\n",
    "    # Reconstruct all rules and parse by seeds\n",
    "    if \"counts\" in lst_data[0][\"metrics\"]:\n",
    "        raise NotImplementedError(\"IS is not supported anymore\")\n",
    "        # Parse IS\n",
    "        rules = []\n",
    "        likelihoods = []\n",
    "        transition_likelihoods = []\n",
    "        weights = []\n",
    "        for data in lst_data:\n",
    "            rules.append(np.repeat(data[\"rules\"], data[\"metrics\"][\"counts\"]))\n",
    "            likelihoods.append(\n",
    "                np.repeat(\n",
    "                    data[\"metrics\"][\"test_likelihoods\"], data[\"metrics\"][\"counts\"]\n",
    "                )\n",
    "            )\n",
    "            weights.append(\n",
    "                np.repeat(data[\"metrics\"][\"weights\"], data[\"metrics\"][\"counts\"])\n",
    "            )\n",
    "            transitions = []\n",
    "            for lst_trajectory_likelihood in data[\"metrics\"][\"test_transition_scores\"]:\n",
    "                transitions.append([transition for trajectory_likelihood in lst_trajectory_likelihood for transition in trajectory_likelihood])\n",
    "            transition_likelihoods.append(np.repeat(transitions, data[\"metrics\"][\"counts\"]))\n",
    "                \n",
    "        all_rules[b_name] = np.stack(rules)\n",
    "        all_likelihoods[b_name] = np.stack(likelihoods)\n",
    "        all_transition_likelihoods[b_name] = np.stack(transition_likelihoods)\n",
    "        all_weights[b_name] = np.stack(weights)\n",
    "    elif \"prev_rules_ind\" in lst_data[0][\"metrics\"]:\n",
    "        # Parse Metropolis\n",
    "        # Reconstruct all rules M\n",
    "        reshaped_rules = []\n",
    "        reshaped_likelihoods = []\n",
    "        reshaped_best_rules = []\n",
    "        reshaped_best_likelihood = []\n",
    "        reshaped_best_transition_ll = []\n",
    "        reshaped_weights = []\n",
    "        reshaped_prev_rules_ind = []\n",
    "        for data in lst_data:\n",
    "            nb_particles = data[\"metrics\"][\"nb_rules\"]\n",
    "            rules = np.array(data[\"rules\"])\n",
    "            likelihoods = np.array(data[\"metrics\"][\"likelihoods\"])\n",
    "            weights = np.array(data[\"metrics\"][\"weights\"])\n",
    "            prev_rules_ind = np.array(data[\"metrics\"][\"prev_rules_ind\"])\n",
    "            best_rules = np.array(data[\"metrics\"][\"best_rule\"])\n",
    "            best_rule_test_ll = np.array(data[\"metrics\"][\"test_likelihoods_best\"])\n",
    "            best_rule_test_transition_ll = []\n",
    "            for lst_trajectory_likelihood in data[\"metrics\"][\"test_transition_scores_best\"]:\n",
    "                best_rule_test_transition_ll.append([transition for trajectory_likelihood in lst_trajectory_likelihood for transition in trajectory_likelihood])\n",
    "            assert len([transition for trajectory_likelihood in lst_trajectory_likelihood for transition in trajectory_likelihood]) == nb_transitions\n",
    "            #Reshape to consider particle as different seeds\n",
    "            reshaped_rules.append(rules.reshape((-1, nb_particles)).transpose())\n",
    "            reshaped_likelihoods.append(\n",
    "                likelihoods.reshape((-1, nb_particles)).transpose()\n",
    "            )\n",
    "            reshaped_weights.append(weights.reshape((-1, nb_particles)).transpose())\n",
    "            reshaped_prev_rules_ind.append(\n",
    "                prev_rules_ind.reshape((-1, nb_particles)).transpose()\n",
    "            )\n",
    "            reshaped_best_rules.append(best_rules.reshape((-1, nb_particles)).transpose())\n",
    "            reshaped_best_likelihood.append(best_rule_test_ll.reshape((-1, nb_particles)).transpose())\n",
    "            reshaped_best_transition_ll.append(\n",
    "                np.array(best_rule_test_transition_ll).reshape((-1,nb_particles,nb_transitions)).transpose((1,0,2))\n",
    "            )\n",
    "        # Concatenate all seeds\n",
    "        all_rules[b_name] = np.concatenate(reshaped_rules, axis=0)\n",
    "        all_likelihoods[b_name] = np.concatenate(reshaped_likelihoods, axis=0)\n",
    "        all_weights[b_name] = np.concatenate(reshaped_weights, axis=0)\n",
    "        all_additional_data[b_name] = {\n",
    "            \"prev_rules_ind\": np.concatenate(reshaped_prev_rules_ind, axis=0)\n",
    "        }\n",
    "        all_best_rules[b_name] = np.concatenate(reshaped_best_rules, axis=0)\n",
    "        all_best_likelihoods[b_name] = np.concatenate(reshaped_best_likelihood, axis=0)\n",
    "        all_best_transition_ll[b_name] = np.concatenate(reshaped_best_transition_ll, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown format\")\n",
    "    all_data[b_name] = lst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rules[\"ALP\"][0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_transition = {\"standing\": [], \"holding1\": [], \"holding2\": [], \"transformP\": [], \"transformSH\": [], \"transformBH\": [], \"nothing\": []}\n",
    "for incr,transition in enumerate(test_observations):\n",
    "    if \"standing\" in transition:\n",
    "        dict_index_transition[\"standing\"].append(incr)\n",
    "    elif \"holding\" in transition:\n",
    "        if \" and \" in transition:\n",
    "            dict_index_transition[\"holding2\"].append(incr)\n",
    "        else:\n",
    "            dict_index_transition[\"holding1\"].append(incr)\n",
    "    elif \"transform\" in transition:\n",
    "        if \"into the carrot\" in transition or \"into the potato.\" in transition or \"into the beet.\" in transition or \"into the berry.\" in transition or \"into the pea.\" in transition:\n",
    "            dict_index_transition[\"transformP\"].append(incr)\n",
    "        elif \"into the pig.\" in transition or \"into the cow.\" in transition or \"into the sheep.\" in transition:\n",
    "            dict_index_transition[\"transformSH\"].append(incr)\n",
    "        elif \"into the elephant.\" in transition or \"into the giraffe.\" in transition or \"into the rhinoceros.\" in transition:\n",
    "            dict_index_transition[\"transformBH\"].append(incr)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown transform\")\n",
    "    elif \"Nothing\" in transition:\n",
    "        dict_index_transition[\"nothing\"].append(incr)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown transition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get repartition of the actions:\n",
    "for action in dict_index_transition:\n",
    "    print(action, len(dict_index_transition[action]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best kept rules of each method\n",
    "best_taken = 50\n",
    "all_baselines_name = list(all_likelihoods.keys())\n",
    "all_best_rules_ind = []\n",
    "for b_name in all_baselines_name:\n",
    "    best_rules_ind = np.argsort(-all_best_likelihoods[b_name].flatten())[:best_taken]\n",
    "    all_best_rules_ind.append(best_rules_ind)\n",
    "likelihoods = np.concatenate([all_best_likelihoods[b_name].flatten()[all_best_rules_ind[incr]] for incr, b_name in enumerate(all_baselines_name)])\n",
    "rules = np.concatenate([all_best_rules[b_name].flatten()[all_best_rules_ind[incr]] for incr, b_name in enumerate(all_baselines_name)])\n",
    "indices = np.argsort(-likelihoods)\n",
    "all_colors_start = [\"\\033[31m\", \"\\033[32m\", \"\\033[34m\", \"\\033[35m\", \"\\033[36m\", \"\\033[37m\", \"\\033[33m\", \"\\033[90m\"]\n",
    "all_colors_end = [\"\\033[0m\"] * len(all_colors_start)\n",
    "all_colors_names = [\"Red\", \"Green\", \"Blue\", \"Magenta\", \"Cyan\", \"White\", \"Yellow\", \"Bright Black\"]\n",
    "# Print colors for every baselines:\n",
    "print(\"Colors for each baselines:\")\n",
    "for incr, b_name in enumerate(all_baselines_name):\n",
    "    print(f\"Baseline: '{b_name}' Color: {all_colors_start[incr]}{all_colors_names[incr]}{all_colors_end[incr]}\")\n",
    "for incr, ind in enumerate(indices):\n",
    "    color_start = all_colors_start[ind//best_taken]\n",
    "    color_end = all_colors_end[ind//best_taken]\n",
    "    print(\n",
    "        f\"{color_start}-----rule-----:{incr}: {repr(rules[ind])}, likelihood: {likelihoods[ind]:2f}{color_end}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sames rules per particle\n",
    "for b_name in all_baselines_name:\n",
    "    redudant_rules = [len(all_rules[b_name][particle])- len(set(all_rules[b_name][particle])) for particle in range(all_rules[b_name].shape[0])]\n",
    "    print(f\"Number of same rules per particle for {b_name}: {redudant_rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison across seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot violin plot of the log likelihood as a function of the seeds for both algorithms\n",
    "for b_name in all_baselines_name[1:]:\n",
    "    plt.figure()\n",
    "    color1 = plt.violinplot(all_best_likelihoods[all_baselines_name[0]].transpose(), showmeans=True, side = \"low\")[\"bodies\"][0].get_facecolor().flatten()\n",
    "    color1 = matplotlib.patches.Patch(color=color1)\n",
    "    color2 = plt.violinplot(all_best_likelihoods[b_name].transpose(), showmeans=True, side=\"high\", positions=[i+1.03 for i in range(len(all_best_likelihoods[b_name]))])[\"bodies\"][0].get_facecolor().flatten()\n",
    "    color2 = matplotlib.patches.Patch(color=color2)\n",
    "    plt.legend([color1, color2], [all_baselines_name[0], b_name])\n",
    "    plt.title(\"Violin plot of the log likelihood for the differents seeds\")\n",
    "    plt.xlabel(\"Seed\")\n",
    "    plt.ylabel(\"Log likelihood\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison collecte de donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transi_type, value in dict_index_transition.items():\n",
    "    print(transi_type +\": \"+ str(len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_size = 10\n",
    "kernel = np.ones(average_size) / average_size\n",
    "for transi_type in dict_index_transition:\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    for incr,b_name in enumerate(all_baselines_name):\n",
    "        # Get the best rules for each particle and data collection for each traj type\n",
    "        _ll_per_transition = all_best_transition_ll[b_name][:,:,dict_index_transition[transi_type]].mean(axis=-1)\n",
    "        \n",
    "        #Add no rule as first collect:\n",
    "        _norule_likelihood_transition = np.array(norule_transition_likelihood_lst)[:,dict_index_transition[transi_type]].mean(axis=-1)\n",
    "        _norule_likelihood_transition = np.repeat(_norule_likelihood_transition, _ll_per_transition.shape[0], axis=0)\n",
    "        average_mean_ll = np.convolve(_ll_per_transition.mean(axis=0), kernel, mode='valid')[::average_size]\n",
    "        average_std_ll = np.convolve(_ll_per_transition.std(axis=0), kernel, mode='valid')[::average_size]\n",
    "        average_mean_ll = np.insert(average_mean_ll,0, _norule_likelihood_transition.mean(axis=0), axis=0)\n",
    "        average_std_ll = np.insert(average_std_ll,0, _norule_likelihood_transition.std(axis=0), axis=0)\n",
    "        #Plot\n",
    "        plt.errorbar(average_size*np.arange(len(average_mean_ll))+ incr*0.05, average_mean_ll, yerr=average_std_ll, label=b_name, capsize=2, fmt='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of data collection\")\n",
    "    plt.ylabel(\"Average log likelihood\")\n",
    "    plt.title(f\"{transi_type}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_size = 10\n",
    "kernel = np.ones(average_size) / average_size\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.subplot(2,3,1)\n",
    "for incr_m, b_name in enumerate(all_baselines_name):\n",
    "    plt.subplot(2,3,incr_m+1)\n",
    "    for incr, transi_type in enumerate(dict_index_transition):\n",
    "        # Get the best rules for each particle and data collection for each traj type\n",
    "        _ll_per_transition = all_best_transition_ll[b_name][:,:,dict_index_transition[transi_type]].mean(axis=-1)\n",
    "        \n",
    "        #Add no rule as first collect:\n",
    "        _norule_likelihood_transition = np.array(norule_transition_likelihood_lst)[:,dict_index_transition[transi_type]].mean(axis=-1)\n",
    "        _norule_likelihood_transition = np.repeat(_norule_likelihood_transition, _ll_per_transition.shape[0], axis=0)\n",
    "        average_mean_ll = np.convolve(_ll_per_transition.mean(axis=0), kernel, mode='valid')[::average_size]\n",
    "        average_std_ll = np.convolve(_ll_per_transition.std(axis=0), kernel, mode='valid')[::average_size]\n",
    "        average_mean_ll = np.insert(average_mean_ll,0, _norule_likelihood_transition.mean(axis=0), axis=0)\n",
    "        average_std_ll = np.insert(average_std_ll,0, _norule_likelihood_transition.std(axis=0), axis=0)\n",
    "        #Plot\n",
    "        plt.errorbar(average_size*np.arange(len(average_mean_ll))+ incr*0.05, average_mean_ll, yerr=average_std_ll, label=transi_type, capsize=2, fmt='o')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of data collection\")\n",
    "    plt.ylim((-36,0))\n",
    "    plt.ylabel(\"Average log likelihood\")\n",
    "    plt.title(f\"Average log likelihood of different transitions types for baseline {b_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10).reshape((5,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(np.concatenate([all_rules[b_name][:, ::20].flatten() for b_name in all_baselines_name], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the t-SNE embeddings\n",
    "proj_embeddings = TSNE(n_components=2).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive backend\n",
    "%matplotlib widget\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(15,10))\n",
    "rules = np.concatenate([all_rules[b_name][:, ::20].flatten() for b_name in all_baselines_name], axis=0)\n",
    "start_index = 0\n",
    "for incr, b_name in enumerate(all_baselines_name):\n",
    "    plt.scatter(proj_embeddings[start_index:start_index+len(all_rules[b_name][:, ::20].flatten()),0], proj_embeddings[start_index:start_index+len(all_rules[b_name][:, ::20].flatten()),1], label=b_name)\n",
    "    start_index = start_index + len(all_rules[b_name][:, ::20].flatten())\n",
    "    \n",
    "# Add interactive cursor\n",
    "cursor = mplcursors.cursor(hover=True)\n",
    "# Define the hover function\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    sel.annotation.set(text=f\"Rule: {(rules[sel.index])}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"TSNE 1\")\n",
    "plt.ylabel(\"TSNE 2\")\n",
    "plt.title(\"TSNE of the rules depending on the generation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings with likelihoods high enough\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(15,10))\n",
    "likelihoods = np.concatenate([all_likelihoods[b_name][:, ::20].flatten() for b_name in all_baselines_name])\n",
    "mask_likelihood = likelihoods > -700\n",
    "start_index = 0\n",
    "for incr, b_name in enumerate(all_baselines_name):\n",
    "    mask_index = (np.arange(len(likelihoods))<start_index+len(all_rules[b_name][:, ::20].flatten())) & (np.arange(len(likelihoods))>=start_index)\n",
    "    plt.scatter(proj_embeddings[mask_index & mask_likelihood,0], proj_embeddings[mask_index & mask_likelihood,1], label=b_name)\n",
    "    start_index = start_index + len(all_rules[b_name][:, ::20].flatten())\n",
    "plt.legend()\n",
    "plt.xlabel(\"TSNE 1\")\n",
    "plt.ylabel(\"TSNE 2\")\n",
    "plt.title(\"TSNE of the rules depending on the generation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the PCA embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "proj_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enable interactive backend\n",
    "# %matplotlib widget\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(15,10))\n",
    "rules = np.concatenate([all_rules[b_name][:, ::20].flatten() for b_name in all_baselines_name], axis=0)\n",
    "start_index = 0\n",
    "for incr, b_name in enumerate(all_baselines_name):\n",
    "    plt.scatter(proj_embeddings[start_index:start_index+len(all_rules[b_name][:, ::20].flatten()),0], proj_embeddings[start_index:start_index+len(all_rules[b_name][:, ::20].flatten()),1], label=b_name)\n",
    "    start_index = start_index + len(all_rules[b_name][:, ::20].flatten())\n",
    "    \n",
    "# Add interactive cursor\n",
    "cursor = mplcursors.cursor(hover=True)\n",
    "# Define the hover function\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    sel.annotation.set(text=f\"Rule: {(rules[sel.index])}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"PCA of the rules depending on the generation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings with likelihoods high enough\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(15,10))\n",
    "likelihoods = np.concatenate([all_likelihoods[b_name][:, ::20].flatten() for b_name in all_baselines_name])\n",
    "mask_likelihood = likelihoods > -720\n",
    "start_index = 0\n",
    "for incr, b_name in enumerate(all_baselines_name):\n",
    "    mask_index = (np.arange(len(likelihoods))<start_index+len(all_rules[b_name][:, ::20].flatten())) & (np.arange(len(likelihoods))>=start_index)\n",
    "    plt.scatter(proj_embeddings[mask_index & mask_likelihood,0], proj_embeddings[mask_index & mask_likelihood,1], label=b_name)\n",
    "    start_index = start_index + len(all_rules[b_name][:, ::20].flatten())\n",
    "plt.legend()\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"PCA of the rules depending on the generation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
